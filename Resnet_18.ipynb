{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import cv2\n",
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import models\n",
    "from tensorflow.keras import optimizers\n",
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>waferMap</th>\n",
       "      <th>dieSize</th>\n",
       "      <th>lotName</th>\n",
       "      <th>waferIndex</th>\n",
       "      <th>trianTestLabel</th>\n",
       "      <th>failureType</th>\n",
       "      <th>wafer_size</th>\n",
       "      <th>failureNum</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...</td>\n",
       "      <td>1683.0</td>\n",
       "      <td>lot1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>none</td>\n",
       "      <td>(45, 48)</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...</td>\n",
       "      <td>1683.0</td>\n",
       "      <td>lot1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>none</td>\n",
       "      <td>(45, 48)</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...</td>\n",
       "      <td>1683.0</td>\n",
       "      <td>lot1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0</td>\n",
       "      <td>none</td>\n",
       "      <td>(45, 48)</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...</td>\n",
       "      <td>1683.0</td>\n",
       "      <td>lot1</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0</td>\n",
       "      <td>none</td>\n",
       "      <td>(45, 48)</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...</td>\n",
       "      <td>1683.0</td>\n",
       "      <td>lot1</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0</td>\n",
       "      <td>none</td>\n",
       "      <td>(45, 48)</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            waferMap  dieSize lotName  \\\n",
       "0  [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...   1683.0    lot1   \n",
       "1  [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...   1683.0    lot1   \n",
       "2  [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...   1683.0    lot1   \n",
       "3  [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...   1683.0    lot1   \n",
       "4  [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...   1683.0    lot1   \n",
       "\n",
       "   waferIndex  trianTestLabel failureType wafer_size  failureNum  \n",
       "0         1.0               0        none   (45, 48)           8  \n",
       "1         2.0               0        none   (45, 48)           8  \n",
       "2         3.0               0        none   (45, 48)           8  \n",
       "3         4.0               0        none   (45, 48)           8  \n",
       "4         5.0               0        none   (45, 48)           8  "
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df=pd.read_pickle(\"./train_set.pkl\")\n",
    "test_df=pd.read_pickle(\"./test_set_all.pkl\")\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>waferMap</th>\n",
       "      <th>dieSize</th>\n",
       "      <th>lotName</th>\n",
       "      <th>waferIndex</th>\n",
       "      <th>trianTestLabel</th>\n",
       "      <th>wafer_size</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2,...</td>\n",
       "      <td>533.0</td>\n",
       "      <td>lot40328</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>(26, 26)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 1, 1, 1,...</td>\n",
       "      <td>533.0</td>\n",
       "      <td>lot40328</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1</td>\n",
       "      <td>(26, 26)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2,...</td>\n",
       "      <td>533.0</td>\n",
       "      <td>lot40328</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1</td>\n",
       "      <td>(26, 26)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 1, 1, 2,...</td>\n",
       "      <td>533.0</td>\n",
       "      <td>lot40328</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1</td>\n",
       "      <td>(26, 26)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 1, 1, 1,...</td>\n",
       "      <td>533.0</td>\n",
       "      <td>lot40328</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1</td>\n",
       "      <td>(26, 26)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            waferMap  dieSize   lotName  \\\n",
       "0  [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2,...    533.0  lot40328   \n",
       "1  [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 1, 1, 1,...    533.0  lot40328   \n",
       "2  [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2,...    533.0  lot40328   \n",
       "3  [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 1, 1, 2,...    533.0  lot40328   \n",
       "4  [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 1, 1, 1,...    533.0  lot40328   \n",
       "\n",
       "   waferIndex  trianTestLabel wafer_size  \n",
       "0         1.0               1   (26, 26)  \n",
       "1         2.0               1   (26, 26)  \n",
       "2         3.0               1   (26, 26)  \n",
       "3         4.0               1   (26, 26)  \n",
       "4         5.0               1   (26, 26)  "
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8    36730\n",
       "3     8554\n",
       "0     3462\n",
       "2     2417\n",
       "4     1620\n",
       "5      609\n",
       "6      500\n",
       "1      409\n",
       "7       54\n",
       "Name: failureNum, dtype: int64"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df[\"failureNum\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25, 27)    15684\n",
       "(27, 25)     9235\n",
       "(26, 26)     6369\n",
       "(38, 36)     1877\n",
       "(33, 37)     1804\n",
       "            ...  \n",
       "(67, 65)        1\n",
       "(63, 63)        1\n",
       "(52, 53)        1\n",
       "(56, 63)        1\n",
       "(53, 44)        1\n",
       "Name: wafer_size, Length: 335, dtype: int64"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df[\"wafer_size\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(45, 48)\n",
      "(45, 48)\n"
     ]
    }
   ],
   "source": [
    "print(train_df[\"waferMap\"][1].shape)\n",
    "print(train_df[\"wafer_size\"][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_preprocessing(x_data, size, num_category, is_test = False, y_data = None):\n",
    "    x = np.zeros((len(x_data), size, size, 1), dtype=np.float32)\n",
    "    if is_test == False:\n",
    "      y = np.zeros((len(x_data),num_category), dtype=np.uint8)\n",
    "      for i, img in enumerate(x_data):\n",
    "        x[i, :, :, :] = np.expand_dims(cv2.resize(img,(size, size)), axis=-1)\n",
    "        y[i,y_data[i]] = 1\n",
    "\n",
    "      return x, y\n",
    "    else:\n",
    "      for i, img in enumerate(x_data):\n",
    "        x[i, :, :] = np.expand_dims(cv2.resize(img,(size, size)), axis=-1)\n",
    "      \n",
    "      return x\n",
    "\n",
    "resize = 64\n",
    "train_x, train_y = data_preprocessing(train_df[\"waferMap\"].to_numpy(), resize, 9, False, train_df[\"failureNum\"].to_numpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x23e1615d130>"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(train_x[5].astype(np.int32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(43484, 64, 64, 1) (43484, 9)\n",
      "(10871, 64, 64, 1) (10871, 9)\n"
     ]
    }
   ],
   "source": [
    "val_split = 0.2\n",
    "num_val = math.floor(len(train_y)*val_split)\n",
    "indice = np.arange(train_x.shape[0])\n",
    "np.random.shuffle(indice)\n",
    "train_x = train_x[indice]/255\n",
    "train_y = train_y[indice]\n",
    "partial_train_x = train_x[:-num_val]\n",
    "val_x = train_x[-num_val:]\n",
    "partial_train_y = train_y[:-num_val]\n",
    "val_y = train_y[-num_val:]\n",
    "print(partial_train_x.shape,partial_train_y.shape)\n",
    "print(val_x.shape,val_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lr_schedule(epoch):\n",
    "    \"\"\"Learning Rate Schedule\n",
    "\n",
    "    Learning rate is scheduled to be reduced after 80, 120, 160, 180 epochs.\n",
    "    Called automatically every epoch as part of callbacks during training.\n",
    "\n",
    "    # Arguments\n",
    "        epoch (int): The number of epochs\n",
    "\n",
    "    # Returns\n",
    "        lr (float32): learning rate\n",
    "    \"\"\"\n",
    "    lr = 1e-3\n",
    "    if epoch > 180:\n",
    "        lr *= 0.5e-3\n",
    "    elif epoch > 160:\n",
    "        lr *= 1e-3\n",
    "    elif epoch > 120:\n",
    "        lr *= 1e-2\n",
    "    elif epoch > 80:\n",
    "        lr *= 1e-1\n",
    "    return lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Bottleneck(in_planes, planes, expansion, x, stride = 1):\n",
    "    residual = x\n",
    "    x = layers.Conv2D(planes, (1, 1), strides = 1, padding = \"same\", use_bias=False)(x)\n",
    "    x = layers.BatchNormalization(trainable = False)(x)\n",
    "    x = layers.Activation('relu')(x)\n",
    "\n",
    "    x = layers.Conv2D(planes, (3, 3), strides = stride, padding = \"same\", use_bias=False)(x)\n",
    "    x = layers.BatchNormalization(trainable = False)(x)\n",
    "    x = layers.Activation('relu')(x)\n",
    "\n",
    "    x = layers.Conv2D(expansion*planes, (3, 3), strides = 1, padding = \"same\", use_bias=False)(x)\n",
    "    x = layers.BatchNormalization(trainable = False)(x)\n",
    "    \n",
    "    if stride != 1 or in_planes != expansion*planes:\n",
    "        residual = layers.Conv2D(expansion*planes, (1, 1), strides = stride, padding = \"same\", use_bias=False)(residual)\n",
    "        residual = layers.BatchNormalization(trainable = False)(residual)\n",
    "\n",
    "    x = layers.add([x, residual])\n",
    "    x = layers.Activation('relu')(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model():\n",
    "    inputs = keras.Input(shape=(64, 64, 1))\n",
    "    x = layers.Conv2D(filters=32, kernel_size=3, padding=\"same\", use_bias=False)(inputs)\n",
    "\n",
    "    x = Bottleneck(32, 16, 4, x)\n",
    "    x = Bottleneck(64, 16, 4, x)\n",
    "\n",
    "    x = Bottleneck(64, 32, 4, x, 2)\n",
    "    x = Bottleneck(128, 32, 4, x)\n",
    "    \n",
    "    x = Bottleneck(128, 64, 4, x, 2)\n",
    "    x = Bottleneck(256, 64, 4, x)\n",
    "\n",
    "    x = Bottleneck(256, 128, 4, x, 2)\n",
    "    x = Bottleneck(512, 128, 4, x)\n",
    "\n",
    "    x = layers.GlobalAveragePooling2D()(x)\n",
    "    x = layers.Dropout(0.5)(x)\n",
    "    outputs = layers.Dense(9, activation=\"softmax\")(x)\n",
    "\n",
    "    return keras.Model(inputs=inputs, outputs=outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_4\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_5 (InputLayer)            [(None, 64, 64, 1)]  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_104 (Conv2D)             (None, 64, 64, 32)   288         input_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_105 (Conv2D)             (None, 64, 64, 16)   512         conv2d_104[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_100 (BatchN (None, 64, 64, 16)   64          conv2d_105[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_84 (Activation)      (None, 64, 64, 16)   0           batch_normalization_100[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_106 (Conv2D)             (None, 64, 64, 16)   2304        activation_84[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_101 (BatchN (None, 64, 64, 16)   64          conv2d_106[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_85 (Activation)      (None, 64, 64, 16)   0           batch_normalization_101[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_107 (Conv2D)             (None, 64, 64, 64)   9216        activation_85[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_108 (Conv2D)             (None, 64, 64, 64)   2048        conv2d_104[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_102 (BatchN (None, 64, 64, 64)   256         conv2d_107[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_103 (BatchN (None, 64, 64, 64)   256         conv2d_108[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "add_28 (Add)                    (None, 64, 64, 64)   0           batch_normalization_102[0][0]    \n",
      "                                                                 batch_normalization_103[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_86 (Activation)      (None, 64, 64, 64)   0           add_28[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_109 (Conv2D)             (None, 64, 64, 16)   1024        activation_86[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_104 (BatchN (None, 64, 64, 16)   64          conv2d_109[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_87 (Activation)      (None, 64, 64, 16)   0           batch_normalization_104[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_110 (Conv2D)             (None, 64, 64, 16)   2304        activation_87[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_105 (BatchN (None, 64, 64, 16)   64          conv2d_110[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_88 (Activation)      (None, 64, 64, 16)   0           batch_normalization_105[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_111 (Conv2D)             (None, 64, 64, 64)   9216        activation_88[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_106 (BatchN (None, 64, 64, 64)   256         conv2d_111[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "add_29 (Add)                    (None, 64, 64, 64)   0           batch_normalization_106[0][0]    \n",
      "                                                                 activation_86[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_89 (Activation)      (None, 64, 64, 64)   0           add_29[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_112 (Conv2D)             (None, 64, 64, 32)   2048        activation_89[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_107 (BatchN (None, 64, 64, 32)   128         conv2d_112[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_90 (Activation)      (None, 64, 64, 32)   0           batch_normalization_107[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_113 (Conv2D)             (None, 32, 32, 32)   9216        activation_90[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_108 (BatchN (None, 32, 32, 32)   128         conv2d_113[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_91 (Activation)      (None, 32, 32, 32)   0           batch_normalization_108[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_114 (Conv2D)             (None, 32, 32, 128)  36864       activation_91[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_115 (Conv2D)             (None, 32, 32, 128)  8192        activation_89[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_109 (BatchN (None, 32, 32, 128)  512         conv2d_114[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_110 (BatchN (None, 32, 32, 128)  512         conv2d_115[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "add_30 (Add)                    (None, 32, 32, 128)  0           batch_normalization_109[0][0]    \n",
      "                                                                 batch_normalization_110[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_92 (Activation)      (None, 32, 32, 128)  0           add_30[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_116 (Conv2D)             (None, 32, 32, 32)   4096        activation_92[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_111 (BatchN (None, 32, 32, 32)   128         conv2d_116[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_93 (Activation)      (None, 32, 32, 32)   0           batch_normalization_111[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_117 (Conv2D)             (None, 32, 32, 32)   9216        activation_93[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_112 (BatchN (None, 32, 32, 32)   128         conv2d_117[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_94 (Activation)      (None, 32, 32, 32)   0           batch_normalization_112[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_118 (Conv2D)             (None, 32, 32, 128)  36864       activation_94[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_113 (BatchN (None, 32, 32, 128)  512         conv2d_118[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "add_31 (Add)                    (None, 32, 32, 128)  0           batch_normalization_113[0][0]    \n",
      "                                                                 activation_92[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_95 (Activation)      (None, 32, 32, 128)  0           add_31[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_119 (Conv2D)             (None, 32, 32, 64)   8192        activation_95[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_114 (BatchN (None, 32, 32, 64)   256         conv2d_119[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_96 (Activation)      (None, 32, 32, 64)   0           batch_normalization_114[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_120 (Conv2D)             (None, 16, 16, 64)   36864       activation_96[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_115 (BatchN (None, 16, 16, 64)   256         conv2d_120[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_97 (Activation)      (None, 16, 16, 64)   0           batch_normalization_115[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_121 (Conv2D)             (None, 16, 16, 256)  147456      activation_97[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_122 (Conv2D)             (None, 16, 16, 256)  32768       activation_95[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_116 (BatchN (None, 16, 16, 256)  1024        conv2d_121[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_117 (BatchN (None, 16, 16, 256)  1024        conv2d_122[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "add_32 (Add)                    (None, 16, 16, 256)  0           batch_normalization_116[0][0]    \n",
      "                                                                 batch_normalization_117[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_98 (Activation)      (None, 16, 16, 256)  0           add_32[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_123 (Conv2D)             (None, 16, 16, 64)   16384       activation_98[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_118 (BatchN (None, 16, 16, 64)   256         conv2d_123[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_99 (Activation)      (None, 16, 16, 64)   0           batch_normalization_118[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_124 (Conv2D)             (None, 16, 16, 64)   36864       activation_99[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_119 (BatchN (None, 16, 16, 64)   256         conv2d_124[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_100 (Activation)     (None, 16, 16, 64)   0           batch_normalization_119[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_125 (Conv2D)             (None, 16, 16, 256)  147456      activation_100[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_120 (BatchN (None, 16, 16, 256)  1024        conv2d_125[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "add_33 (Add)                    (None, 16, 16, 256)  0           batch_normalization_120[0][0]    \n",
      "                                                                 activation_98[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_101 (Activation)     (None, 16, 16, 256)  0           add_33[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_126 (Conv2D)             (None, 16, 16, 128)  32768       activation_101[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_121 (BatchN (None, 16, 16, 128)  512         conv2d_126[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_102 (Activation)     (None, 16, 16, 128)  0           batch_normalization_121[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_127 (Conv2D)             (None, 8, 8, 128)    147456      activation_102[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_122 (BatchN (None, 8, 8, 128)    512         conv2d_127[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_103 (Activation)     (None, 8, 8, 128)    0           batch_normalization_122[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_128 (Conv2D)             (None, 8, 8, 512)    589824      activation_103[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_129 (Conv2D)             (None, 8, 8, 512)    131072      activation_101[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_123 (BatchN (None, 8, 8, 512)    2048        conv2d_128[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_124 (BatchN (None, 8, 8, 512)    2048        conv2d_129[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "add_34 (Add)                    (None, 8, 8, 512)    0           batch_normalization_123[0][0]    \n",
      "                                                                 batch_normalization_124[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_104 (Activation)     (None, 8, 8, 512)    0           add_34[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_130 (Conv2D)             (None, 8, 8, 128)    65536       activation_104[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_125 (BatchN (None, 8, 8, 128)    512         conv2d_130[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_105 (Activation)     (None, 8, 8, 128)    0           batch_normalization_125[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_131 (Conv2D)             (None, 8, 8, 128)    147456      activation_105[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_126 (BatchN (None, 8, 8, 128)    512         conv2d_131[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_106 (Activation)     (None, 8, 8, 128)    0           batch_normalization_126[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_132 (Conv2D)             (None, 8, 8, 512)    589824      activation_106[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_127 (BatchN (None, 8, 8, 512)    2048        conv2d_132[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "add_35 (Add)                    (None, 8, 8, 512)    0           batch_normalization_127[0][0]    \n",
      "                                                                 activation_104[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_107 (Activation)     (None, 8, 8, 512)    0           add_35[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d_4 (Glo (None, 512)          0           activation_107[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, 512)          0           global_average_pooling2d_4[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 9)            4617        dropout_4[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 2,283,305\n",
      "Trainable params: 2,267,945\n",
      "Non-trainable params: 15,360\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = build_model()\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=optimizers.Adam(learning_rate=lr_schedule(0)),\n",
    "              metrics=['acc'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "340/340 [==============================] - 25s 70ms/step - loss: 0.8718 - acc: 0.7537 - val_loss: 0.4735 - val_acc: 0.8629\n",
      "Epoch 2/100\n",
      "340/340 [==============================] - 23s 69ms/step - loss: 0.4244 - acc: 0.8753 - val_loss: 0.4013 - val_acc: 0.8795\n",
      "Epoch 3/100\n",
      "340/340 [==============================] - 23s 69ms/step - loss: 0.3841 - acc: 0.8885 - val_loss: 0.3813 - val_acc: 0.8879\n",
      "Epoch 4/100\n",
      "340/340 [==============================] - 23s 69ms/step - loss: 0.3590 - acc: 0.8976 - val_loss: 0.3533 - val_acc: 0.8972\n",
      "Epoch 5/100\n",
      "340/340 [==============================] - 24s 69ms/step - loss: 0.3227 - acc: 0.9055 - val_loss: 0.2781 - val_acc: 0.9122\n",
      "Epoch 6/100\n",
      "340/340 [==============================] - 24s 69ms/step - loss: 0.2402 - acc: 0.9228 - val_loss: 0.2097 - val_acc: 0.9342\n",
      "Epoch 7/100\n",
      "340/340 [==============================] - 24s 69ms/step - loss: 0.1829 - acc: 0.9411 - val_loss: 0.1778 - val_acc: 0.9438\n",
      "Epoch 8/100\n",
      "340/340 [==============================] - 24s 69ms/step - loss: 0.1478 - acc: 0.9517 - val_loss: 0.1279 - val_acc: 0.9571\n",
      "Epoch 9/100\n",
      "340/340 [==============================] - 24s 69ms/step - loss: 0.1263 - acc: 0.9603 - val_loss: 0.1464 - val_acc: 0.9567\n",
      "Epoch 10/100\n",
      "340/340 [==============================] - 24s 69ms/step - loss: 0.1136 - acc: 0.9643 - val_loss: 0.1135 - val_acc: 0.9641\n",
      "Epoch 11/100\n",
      "340/340 [==============================] - 23s 69ms/step - loss: 0.0944 - acc: 0.9685 - val_loss: 0.1196 - val_acc: 0.9622\n",
      "Epoch 12/100\n",
      "340/340 [==============================] - 23s 69ms/step - loss: 0.0851 - acc: 0.9719 - val_loss: 0.0951 - val_acc: 0.9719\n",
      "Epoch 13/100\n",
      "340/340 [==============================] - 24s 69ms/step - loss: 0.0804 - acc: 0.9742 - val_loss: 0.1086 - val_acc: 0.9679\n",
      "Epoch 14/100\n",
      "340/340 [==============================] - 24s 69ms/step - loss: 0.0812 - acc: 0.9744 - val_loss: 0.1008 - val_acc: 0.9702\n",
      "Epoch 15/100\n",
      "340/340 [==============================] - 24s 69ms/step - loss: 0.0674 - acc: 0.9785 - val_loss: 0.0881 - val_acc: 0.9737\n",
      "Epoch 16/100\n",
      "340/340 [==============================] - 24s 69ms/step - loss: 0.0607 - acc: 0.9798 - val_loss: 0.0882 - val_acc: 0.9757\n",
      "Epoch 17/100\n",
      "340/340 [==============================] - 24s 69ms/step - loss: 0.0513 - acc: 0.9828 - val_loss: 0.0974 - val_acc: 0.9719\n",
      "Epoch 18/100\n",
      "340/340 [==============================] - 24s 69ms/step - loss: 0.0501 - acc: 0.9838 - val_loss: 0.0893 - val_acc: 0.9752\n",
      "Epoch 19/100\n",
      "340/340 [==============================] - 24s 69ms/step - loss: 0.0491 - acc: 0.9846 - val_loss: 0.0938 - val_acc: 0.9746\n",
      "Epoch 20/100\n",
      "340/340 [==============================] - 24s 69ms/step - loss: 0.0459 - acc: 0.9853 - val_loss: 0.0895 - val_acc: 0.9740\n",
      "Epoch 21/100\n",
      "340/340 [==============================] - 24s 69ms/step - loss: 0.0413 - acc: 0.9867 - val_loss: 0.0898 - val_acc: 0.9740\n",
      "Epoch 22/100\n",
      "340/340 [==============================] - 23s 69ms/step - loss: 0.0338 - acc: 0.9892 - val_loss: 0.0815 - val_acc: 0.9778\n",
      "Epoch 23/100\n",
      "340/340 [==============================] - 24s 69ms/step - loss: 0.0325 - acc: 0.9897 - val_loss: 0.1050 - val_acc: 0.9763\n",
      "Epoch 24/100\n",
      "340/340 [==============================] - 24s 69ms/step - loss: 0.0335 - acc: 0.9895 - val_loss: 0.0740 - val_acc: 0.9799\n",
      "Epoch 25/100\n",
      "340/340 [==============================] - 24s 69ms/step - loss: 0.0295 - acc: 0.9904 - val_loss: 0.0817 - val_acc: 0.9779\n",
      "Epoch 26/100\n",
      "340/340 [==============================] - 24s 69ms/step - loss: 0.0304 - acc: 0.9904 - val_loss: 0.0770 - val_acc: 0.9779\n",
      "Epoch 27/100\n",
      "340/340 [==============================] - 24s 69ms/step - loss: 0.0240 - acc: 0.9920 - val_loss: 0.0907 - val_acc: 0.9777\n",
      "Epoch 28/100\n",
      "340/340 [==============================] - 24s 69ms/step - loss: 0.0272 - acc: 0.9914 - val_loss: 0.0945 - val_acc: 0.9800\n",
      "Epoch 29/100\n",
      "340/340 [==============================] - 23s 69ms/step - loss: 0.0229 - acc: 0.9928 - val_loss: 0.0906 - val_acc: 0.9787\n",
      "Epoch 30/100\n",
      "340/340 [==============================] - 23s 69ms/step - loss: 0.0258 - acc: 0.9915 - val_loss: 0.0931 - val_acc: 0.9793\n",
      "Epoch 31/100\n",
      "340/340 [==============================] - 24s 69ms/step - loss: 0.0219 - acc: 0.9932 - val_loss: 0.0990 - val_acc: 0.9804\n",
      "Epoch 32/100\n",
      "340/340 [==============================] - 24s 69ms/step - loss: 0.0214 - acc: 0.9936 - val_loss: 0.0968 - val_acc: 0.9788\n",
      "Epoch 33/100\n",
      "340/340 [==============================] - 24s 69ms/step - loss: 0.0214 - acc: 0.9928 - val_loss: 0.1068 - val_acc: 0.9773\n",
      "Epoch 34/100\n",
      "340/340 [==============================] - 24s 69ms/step - loss: 0.0200 - acc: 0.9935 - val_loss: 0.0984 - val_acc: 0.9787\n",
      "Epoch 35/100\n",
      "340/340 [==============================] - 24s 69ms/step - loss: 0.0180 - acc: 0.9943 - val_loss: 0.1258 - val_acc: 0.9707\n",
      "Epoch 36/100\n",
      "340/340 [==============================] - 24s 69ms/step - loss: 0.0173 - acc: 0.9947 - val_loss: 0.1232 - val_acc: 0.9777\n",
      "Epoch 37/100\n",
      "340/340 [==============================] - 24s 69ms/step - loss: 0.0185 - acc: 0.9943 - val_loss: 0.0907 - val_acc: 0.9795\n",
      "Epoch 38/100\n",
      "340/340 [==============================] - 24s 69ms/step - loss: 0.0172 - acc: 0.9950 - val_loss: 0.0972 - val_acc: 0.9785\n",
      "Epoch 39/100\n",
      "340/340 [==============================] - 24s 69ms/step - loss: 0.0175 - acc: 0.9945 - val_loss: 0.0851 - val_acc: 0.9788\n",
      "Epoch 40/100\n",
      "340/340 [==============================] - 24s 69ms/step - loss: 0.0157 - acc: 0.9951 - val_loss: 0.1117 - val_acc: 0.9811\n",
      "Epoch 41/100\n",
      "340/340 [==============================] - 24s 69ms/step - loss: 0.0154 - acc: 0.9954 - val_loss: 0.0973 - val_acc: 0.9801\n",
      "Epoch 42/100\n",
      "340/340 [==============================] - 24s 69ms/step - loss: 0.0173 - acc: 0.9950 - val_loss: 0.0901 - val_acc: 0.9802\n",
      "Epoch 43/100\n",
      "340/340 [==============================] - 24s 69ms/step - loss: 0.0115 - acc: 0.9965 - val_loss: 0.1133 - val_acc: 0.9774\n",
      "Epoch 44/100\n",
      "340/340 [==============================] - 24s 69ms/step - loss: 0.0180 - acc: 0.9946 - val_loss: 0.1166 - val_acc: 0.9752\n",
      "Epoch 45/100\n",
      "340/340 [==============================] - 24s 69ms/step - loss: 0.0123 - acc: 0.9963 - val_loss: 0.0906 - val_acc: 0.9817\n",
      "Epoch 46/100\n",
      "340/340 [==============================] - 24s 69ms/step - loss: 0.0129 - acc: 0.9960 - val_loss: 0.1198 - val_acc: 0.9799\n",
      "Epoch 47/100\n",
      "340/340 [==============================] - 24s 69ms/step - loss: 0.0170 - acc: 0.9946 - val_loss: 0.1101 - val_acc: 0.9794\n",
      "Epoch 48/100\n",
      "340/340 [==============================] - 24s 69ms/step - loss: 0.0117 - acc: 0.9963 - val_loss: 0.1001 - val_acc: 0.9822\n",
      "Epoch 49/100\n",
      "340/340 [==============================] - 24s 69ms/step - loss: 0.0120 - acc: 0.9967 - val_loss: 0.1126 - val_acc: 0.9816\n",
      "Epoch 50/100\n",
      "340/340 [==============================] - 24s 69ms/step - loss: 0.0092 - acc: 0.9973 - val_loss: 0.1024 - val_acc: 0.9793\n",
      "Epoch 51/100\n",
      "340/340 [==============================] - 24s 69ms/step - loss: 0.0135 - acc: 0.9961 - val_loss: 0.0926 - val_acc: 0.9817\n",
      "Epoch 52/100\n",
      "340/340 [==============================] - 24s 69ms/step - loss: 0.0130 - acc: 0.9962 - val_loss: 0.1029 - val_acc: 0.9798\n",
      "Epoch 53/100\n",
      "340/340 [==============================] - 24s 69ms/step - loss: 0.0146 - acc: 0.9953 - val_loss: 0.0933 - val_acc: 0.9807\n",
      "Epoch 54/100\n",
      "340/340 [==============================] - 24s 69ms/step - loss: 0.0117 - acc: 0.9965 - val_loss: 0.0950 - val_acc: 0.9824\n",
      "Epoch 55/100\n",
      "340/340 [==============================] - 24s 69ms/step - loss: 0.0094 - acc: 0.9974 - val_loss: 0.0947 - val_acc: 0.9793\n",
      "Epoch 56/100\n",
      "340/340 [==============================] - 24s 70ms/step - loss: 0.0112 - acc: 0.9966 - val_loss: 0.0936 - val_acc: 0.9832\n",
      "Epoch 57/100\n",
      "340/340 [==============================] - 24s 69ms/step - loss: 0.0099 - acc: 0.9969 - val_loss: 0.1117 - val_acc: 0.9776\n",
      "Epoch 58/100\n",
      "340/340 [==============================] - 24s 69ms/step - loss: 0.0139 - acc: 0.9961 - val_loss: 0.0925 - val_acc: 0.9818\n",
      "Epoch 59/100\n",
      "340/340 [==============================] - 24s 69ms/step - loss: 0.0072 - acc: 0.9978 - val_loss: 0.1234 - val_acc: 0.9804\n",
      "Epoch 60/100\n",
      "340/340 [==============================] - 24s 69ms/step - loss: 0.0107 - acc: 0.9967 - val_loss: 0.0999 - val_acc: 0.9804\n",
      "Epoch 61/100\n",
      " 37/340 [==>...........................] - ETA: 19s - loss: 0.0078 - acc: 0.9975"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_22724/1480451593.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m history = model.fit(\n\u001b[0m\u001b[0;32m      2\u001b[0m       \u001b[0mpartial_train_x\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mpartial_train_y\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m       \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m128\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m       \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mval_x\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mval_y\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m       \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\Terry\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1181\u001b[0m                 _r=1):\n\u001b[0;32m   1182\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1183\u001b[1;33m               \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1184\u001b[0m               \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1185\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\Terry\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    887\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    888\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 889\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    890\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    891\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\Terry\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    915\u001b[0m       \u001b[1;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    916\u001b[0m       \u001b[1;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 917\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    918\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    919\u001b[0m       \u001b[1;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\Terry\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   3021\u001b[0m       (graph_function,\n\u001b[0;32m   3022\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m-> 3023\u001b[1;33m     return graph_function._call_flat(\n\u001b[0m\u001b[0;32m   3024\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0;32m   3025\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\Terry\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1958\u001b[0m         and executing_eagerly):\n\u001b[0;32m   1959\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1960\u001b[1;33m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[0;32m   1961\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0;32m   1962\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[1;32m~\\anaconda3\\envs\\Terry\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    589\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    590\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 591\u001b[1;33m           outputs = execute.execute(\n\u001b[0m\u001b[0;32m    592\u001b[0m               \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    593\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\Terry\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     57\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 59\u001b[1;33m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[0;32m     60\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[0;32m     61\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "      partial_train_x,partial_train_y,\n",
    "      batch_size=128,\n",
    "      validation_data=(val_x,val_y),\n",
    "      epochs=100,\n",
    "      #verbose = 0\n",
    "      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 8. 12. 36. 10. 54.  5. 47.  1. 18.]\n"
     ]
    }
   ],
   "source": [
    "val_y_preds = model.predict(val_x)\n",
    "error = np.zeros(9)\n",
    "for i,pred in enumerate(val_y_preds):\n",
    "    if val_y[i, np.argmax(pred)] == 0:\n",
    "        error[np.argmax(val_y[i])] += 1\n",
    "print(error)\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "40e8bd7b9004e7ac39e8b6a07bd0b727143e4a03e82b08ba505ad3a9c3223159"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('Terry')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
